#FOR LISTS OF URLS

import urllib.request
import re
import string
import nltk
from nltk.stem import WordNetLemmatizer

def find_beginning_and_end(raw):
    '''
    This function serves to find the text within the raw data provided by Project Gutenberg
    '''
    end_regex = 'end of th(is|e) project gutenberg ebook'
    end_position = re.search(end_regex, raw.lower())
    if end_position:
        text = raw[:end_position.start()]
    else:
        text = raw
    return text

def preprocessing(text):
    lower_text = text.lower()
    no_punct_corpus = lower_text.translate(str.maketrans("", "", string.punctuation))
    no_number_corpus = re.sub(r'\d+', '', no_punct_corpus)
    no_number_corpus = str(no_number_corpus)
    tokenized_corpus = nltk.word_tokenize(no_number_corpus)
    lemmatizer = WordNetLemmatizer()
    first_lemmatized_corpus = " ".join(lemmatizer.lemmatize(word) for word in tokenized_corpus)
    lemmatized_corpus = nltk.word_tokenize(first_lemmatized_corpus)
    return lemmatized_corpus

def process_url(url):
    raw = urllib.request.urlopen(url).read().decode().strip()
    text = find_beginning_and_end(raw)
    processed_text = preprocessing(text)
    return processed_text

urls = [
    "http://www.gutenberg.org/files/2147/2147-0.txt",
    "http://www.gutenberg.org/files/1342/1342-0.txt",
    "http://www.gutenberg.org/files/98/98-0.txt"
]

for url in urls:
    processed_text = process_url(url)
    print("Processed text from URL:", url)
    print(processed_text[:100])
    print("\n")
    
#FOR SINGLE ITEMS (some statements are not used but better to download them before, we'll need them at some point

#by single item

import urllib.request
import re
import string
import nltk
from nltk.stem import WordNetLemmatizer

poeUrl = "http://www.gutenberg.org/files/2147/2147-0.txt"
poeString = urllib.request.urlopen(poeUrl).read().decode().strip()

def find_beginning_and_end(raw):
    '''
    This function serves to find the text within the raw data provided by Project Gutenberg
    '''

    start_regex = '\*\*\*\s?START OF TH(IS|E) PROJECT GUTENBERG EBOOK.*\*\*\*'
    draft_start_position = re.search(start_regex, raw)
    beginning = draft_start_position.end()

    end_regex = 'end of th(is|e) project gutenberg ebook'
    end_position = re.search(end_regex, raw.lower())

    text = raw[beginning:end_position.start()]

    return text

def preprocessing(text):
    lower_text = text.lower()
    no_punct_corpus = lower_text.translate(str.maketrans("", "", string.punctuation))
    no_number_corpus = re.sub(r'\d+', '', no_punct_corpus)
    no_number_corpus = str(no_number_corpus)
    tokenized_corpus = nltk.word_tokenize(no_number_corpus)
    lemmatizer = WordNetLemmatizer()
    lemmatized_corpus = " ".join(lemmatizer.lemmatize(word) for word in tokenized_corpus)
    return lemmatized_corpus

raw_text = poeString
preprocessed_text = nltk.word_tokenize(preprocessing(find_beginning_and_end(raw_text)))
print(preprocessed_text)

